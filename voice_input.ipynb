{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 1.94k/1.94k [00:00<00:00, 4.25MB/s]\n",
      "c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dhruv Kanetkar\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 290M/290M [00:26<00:00, 10.8MB/s] \n",
      "Downloading generation_config.json: 100%|██████████| 1.53k/1.53k [00:00<00:00, 3.05MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 805/805 [00:00<00:00, 793kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 3.68MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.41M/2.41M [00:00<00:00, 6.32MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 5.23MB/s]\n",
      "Downloading normalizer.json: 100%|██████████| 52.7k/52.7k [00:00<00:00, 35.1MB/s]\n",
      "Downloading added_tokens.json: 100%|██████████| 34.6k/34.6k [00:00<00:00, 678kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.83k/1.83k [00:00<?, ?B/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 185k/185k [00:00<00:00, 2.22MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "p = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "e e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\route_utils.py\", line 233, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\blocks.py\", line 1608, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\utils.py\", line 689, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dhruv Kanetkar\\AppData\\Local\\Temp\\ipykernel_1324\\352352443.py\", line 10, in transcribe\n",
      "    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 357, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 461, in preprocess\n",
      "    raise ImportError(\n",
      "ImportError: torchaudio is required to resample audio samples in AutomaticSpeechRecognitionPipeline. The torchaudio package can be installed through: `pip install torchaudio`.\n"
     ]
    }
   ],
   "source": [
    "# Define trivia question packs\n",
    "question_packs = {\n",
    "    \"General Knowledge\": [\n",
    "        {\"question\": \"What is the capital of France?\", \"options\": [\"London\", \"Paris\", \"Berlin\", \"Madrid\"], \"answer\": \"Paris\"},\n",
    "        {\"question\": \"Who wrote Romeo and Juliet?\", \"options\": [\"Charles Dickens\", \"Jane Austen\", \"William Shakespeare\", \"Mark Twain\"], \"answer\": \"William Shakespeare\"},\n",
    "        # Add more questions to the pack\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        {\"question\": \"What is the chemical symbol for water?\", \"options\": [\"H2O\", \"CO2\", \"O2\", \"NaCl\"], \"answer\": \"H2O\"},\n",
    "        {\"question\": \"Who discovered penicillin?\", \"options\": [\"Marie Curie\", \"Louis Pasteur\", \"Alexander Fleming\", \"Robert Koch\"], \"answer\": \"Alexander Fleming\"},\n",
    "        # Add more questions to the pack\n",
    "    ],\n",
    "    # Add more question packs if needed\n",
    "}\n",
    "\n",
    "# Function to get a random question from a selected pack\n",
    "def get_random_question(pack_name):\n",
    "    if pack_name in question_packs:\n",
    "        return random.choice(question_packs[pack_name])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to check if the user's answer is correct\n",
    "def check_answer(answer, correct_answer):\n",
    "    return answer == correct_answer\n",
    "\n",
    "# Function to handle the Gradio interface\n",
    "def trivia_app(question_pack, user_answer):\n",
    "    question_data = get_random_question(question_pack)\n",
    "    if question_data:\n",
    "        question_text = f\"Question: {question_data['question']}\"\n",
    "        options = question_data['options']\n",
    "\n",
    "        # Check if the user's answer is correct\n",
    "        is_correct = check_answer(user_answer, question_data['answer'])\n",
    "\n",
    "        # Provide feedback based on the correctness of the answer\n",
    "        feedback = \"Correct!\" if is_correct else f\"Wrong! The correct answer is {question_data['answer']}\"\n",
    "        return question_text, options, f\"Your answer: {user_answer}\", feedback\n",
    "    else:\n",
    "        return \"Invalid question pack selected.\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=trivia_app,\n",
    "    inputs=[gr.Dropdown(list(question_packs.keys()), label=\"Select Trivia Pack\"),\n",
    "            gr.Radio(list(\"ABCD\"), label=\"Your Answer\")],\n",
    "    outputs=[\"text\", \"text\", \"text\", \"text\"],\n",
    "    live=True,\n",
    "    title=\"Trivia Question App\",\n",
    "    description=\"Select a trivia pack and answer multiple-choice questions.\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\route_utils.py\", line 233, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\blocks.py\", line 1608, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\gradio\\utils.py\", line 689, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dhruv Kanetkar\\AppData\\Local\\Temp\\ipykernel_1324\\4289150799.py\", line 10, in transcribe\n",
      "    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 357, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dhruv Kanetkar\\anaconda3\\envs\\magellan\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 461, in preprocess\n",
      "    raise ImportError(\n",
      "ImportError: torchaudio is required to resample audio samples in AutomaticSpeechRecognitionPipeline. The torchaudio package can be installed through: `pip install torchaudio`.\n"
     ]
    }
   ],
   "source": [
    "def transcribe(stream, new_chunk):\n",
    "    sr, y = new_chunk\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n",
    "    [\"state\", \"text\"],\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of Components\n",
    "\n",
    "## ML Side\n",
    "- Function to convert speech to text\n",
    "- Function to convert text to speech\n",
    "\n",
    "## Game Logic Side\n",
    "- Selector function for game pack\n",
    "- que generator for the questions\n",
    "- selector for the question in each pack\n",
    "- Evalutor function if the answer is right\n",
    "- Function to update streak\n",
    "\n",
    "## Database and State Management\n",
    "- sqlite database:\n",
    "    - table for each pack\n",
    "    - cols: question, answer, choices\n",
    "    - rows: each Q\n",
    "\n",
    "- state management for answered questions\n",
    "- counter for correct answer streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magellan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
